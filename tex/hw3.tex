\documentclass[10pt]{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{amsxtra, amscd, geometry, graphicx}
\usepackage{endnotes}
\usepackage{cancel}
\usepackage{alltt}
%\usepackage[all,cmtip]{xypic}
\usepackage{mathrsfs}
\usepackage{listings}
%\usepackage{subfigure}
\usepackage{subcaption}
%\usepackage[pdftex]{hyperref}
%\usepackage[dvips,bookmarks,bookmarksopen,backref,colorlinks,linkcolor={blue},citecolor={blue},urlcolor={blue}](hyperref}

% Makes the margin size a little smaller.
\geometry{letterpaper,margin=1.3in}

% Possible font packages. Choose one and comment out the rest.
\usepackage{times}%
%\usepackage{helvet}%
%\usepackage{palatino}%
%\usepackage{bookman}%

% These are italic.
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem*{prop*}{Proposition}
\newtheorem{conj}{Conjecture}
\newtheorem*{conj*}{Conjecture}
\newtheorem{lem}{Lemma}
  \makeatletter
  \@addtoreset{lem}{thm}
  \makeatother 
\newtheorem*{lem*}{Lemma}
\newtheorem{cor}{Corollary}
  \makeatletter
  \@addtoreset{cor}{thm}
  \makeatother 
\newtheorem*{cor*}{Corollary}

%\newtheorem{lem}[thm]{Lemma}
%\newtheorem{remark}[thm]{Remark}
%\newtheorem{cor}[thm]{Corollary}
%\newtheorem{prop}[thm]{Proposition}
%\newtheorem{conj}[thm]{Conjecture}

% These are normal (i.e. not italic).
\theoremstyle{definition}
\newtheorem*{ack*}{Acknowledgements}
\newtheorem*{app*}{Application}
\newtheorem*{apps*}{Applications}
\newtheorem{defn}{Definition}
\newtheorem*{defn*}{Definition}
\newtheorem{eg}{Example}
  \makeatletter
  \@addtoreset{eg}{thm}
  \makeatother 
\newtheorem*{eg*}{Example}
\newtheorem*{egs*}{Examples}
\newtheorem{ex}{Exercise}
\newtheorem*{ex*}{Exercise}
\newtheorem*{quest*}{Question}
\newtheorem{rem}{Remark}
\newtheorem*{rem*}{Remark}
\newtheorem{rems}{Remarks}
\newtheorem*{rems*}{Remarks}
\newtheorem{prob}{Problem}
\newtheorem*{prob*}{Problem}
\newtheorem*{soln*}{Solution}
\newtheorem{soln}{Solution}


% New Commands: Common Math Symbols
\providecommand{\R}{\mathbb{R}}%
\providecommand{\N}{\mathbb{N}}%
\providecommand{\Z}{{\mathbb{Z}}}%
\providecommand{\sph}{\mathbb{S}}%
\providecommand{\Q}{\mathbb{Q}}%
\providecommand{\C}{{\mathbb{C}}}%
\providecommand{\F}{\mathbb{F}}%
\providecommand{\quat}{\mathbb{H}}%

% New Commands: Operators
\providecommand{\Gal}{\operatorname{Gal}}%
\providecommand{\GL}{\operatorname{GL}}%
\providecommand{\card}{\operatorname{card}}%
\providecommand{\coker}{\operatorname{coker}}%
\providecommand{\id}{\operatorname{id}}%
\providecommand{\im}{\operatorname{im}}%
\providecommand{\diam}{{\rm diam}}%
\providecommand{\aut}{\operatorname{Aut}}%
\providecommand{\inn}{\operatorname{Inn}}%
\providecommand{\out}{{\rm Out}}%
\providecommand{\End}{{\rm End}}%
\providecommand{\rad}{{\rm Rad}}%
\providecommand{\rk}{{\rm rank}}%
\providecommand{\ord}{{\rm ord}}%
\providecommand{\tor}{{\rm Tor}}%
\providecommand{\comp}{{\text{ $\scriptstyle \circ$ }}}%
\providecommand{\cl}[1]{\overline{#1}}%
\providecommand{\tr}{{\sf trace}}%

\renewcommand{\tilde}[1]{\widetilde{#1}}%
\numberwithin{equation}{section}

\renewcommand{\epsilon}{\varepsilon}

\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}

% This makes the spacing between lines of font a little bigger.  I like the way it looks.
\newcommand{\spacing}[1]{\renewcommand{\baselinestretch}{#1}\large\normalsize}
\spacing{1.2}

% END PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\lstset{language=Python}

\title{\Large{579 HW3}}
\author{Luke Wukmer}
\date{\today}
\maketitle \normalsize \thispagestyle{empty} % remove the page number from the first page

%%%%% PROBLEM 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
\begin{prob}
\end{prob}
\begin{prob*}[\bf{1a}]
\end{prob*}

\begin{proof}
  $f$ convex, so for $ A\vec{w} , A \vec{z} \in \R^{d} $
  \[
    f \left(\theta (A \vec{w}) + (1 - \theta) A \vec{z} \right)
    \; \leq \;
    \theta f( A \vec{w} ) + (1 - \theta) f( A \vec{z} )
  \]
  By linearity of $A$, we can rewrite the left side as
  \[
    f \left(A \left[ \theta \vec{w} + (1 - \theta) \vec{z} \right]\right) 
    \; \leq \;
    \theta f( A \vec{w} ) + (1 - \theta) f( A \vec{z} )
  \]
  Rewriting in terms of $g = f(A\vec(w))$:
  \[
    g \left( \theta \vec{w} + (1 - \theta) \vec{z} \right)
    \; \leq \;
    \theta g(\vec{w}) + (1 - \theta) g(\vec{z})
  \]
  Thus g is convex.
\qedhere  
\end{proof}
%%%% PROBLEM 1B
%%%%%%%%%%%%%%%%%%%%%%%5
\begin{prob*}[\bf{1b}]
\end{prob*}

\begin{proof}
  \[
  \begin{aligned}
    h \left( \theta \vec{x} + (1-\theta) \vec{y} \right)
    &= \sum \alpha_i f_i \left( \theta \vec{x} + (1-\theta) \vec{y} \right) \\
    &\leq \sum \alpha_i \left[ \theta f_i( \vec{x})
                              + (1-\theta) f_i(\vec{y}) \right] \\
    &= \sum \alpha_i \theta f_i( \vec{x})
            + \sum \alpha_i (1-\theta) f_i(\vec{y}) \\
    &= \theta \sum \alpha_i f_i( \vec{x})
            + (1-\theta) \sum \alpha_i  f_i(\vec{y}) \\
    &= \theta h(\vec{x}) + (1-\theta) h(\vec{y}) \\
  \end{aligned}
  \]
  where the inequality follows from the fact that $f_i$ are each convex and 
  $\alpha_i \geq 0$.
\end{proof}
\clearpage
%%%%%% PROBLEM 1C
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{prob*}[\bf{1c}]
\end{prob*}

\begin{proof}
  \[
  \begin{aligned}
    g(z)   &:= \log(1+e^{z}) \\
    g'(z)  &= \frac{e^{z}}{1+e^{z}} \\
    g''(z) &= \frac{ (1+e^{z}) e^{z} - e^{z} (e^{z}) }{(1+e^{z})^2} \\
           &= \frac{ e^{z} }{ (1+e^{z})^2 } \, \geq \, 0
                \quad \forall z \in \R \\
  \end{aligned}
  \]
  \[
  \begin{aligned}
    h(z)   &:= \log(1+e^{-z}) \\
    h'(z)   &= \frac{-e^{z}}{1+e^{-z}} \\
            &= \left(\frac{e^{z}}{e^{z}}\right) \frac{-e^{z}}{1+e^{-z}}
                  = \frac{-1}{e^{z} + 1 }\\
    h''(z)  &= \frac{(e^{z}+1)(0) - (-1)e^{z}}{(e^{z}+1)^2}
            &= \frac{e^{z}}{(e^{z}+1)^2} \, \geq \, 0
                \quad \forall z \in \R \\
  \end{aligned}
  \]

  Thus $g, h$ are each convex in $\R$.
  As $z(\vec{x_i}) \in \R$, we note that
  $g(z(\vec{x_i})), h(z(\vec{x_i}))$ are each convex in $\R$.

  Using the result of (1b) above, we see that
  \[
    E(\alpha, \beta) = \sum g(\vec{x_i}) (1 - y_i) + \sum h(\vec{x_i})y_i
  \]
  is simply a sum of convex function with appropriate scalars, and thus E is
  convex.\\

\end{proof}

%%%%%%PROBLEM 1D
\begin{prob*}[\bf{1d}]
\end{prob*}

\begin{proof}
First suppose that $\nabla f(\vec{x}_{\star}) = 0$.
Then $\forall \ \vec{x} \in \R^d$
\[
	f(\vec{x})-f(\vec{x}_{\star})) \geq
	\langle \nabla f(\vec{x}_{\star}) , \vec{x} - \vec{x}_{\star} \rangle
	= 0
	\]
	\[
	\Rightarrow \quad f(\vec{x}) \geq f(\vec{x}_{\star})
	\]
	\[
	\Rightarrow \quad f(\vec{x}_{\star}) = \min_{\vec{x} \in \R^d} f(\vec{x})
\]
Now suppose $f(\vec{x}_{\star}) = \min f(\vec{x})$. Using the inequality again with choices $x_{\star}$ and $x_{\star} + \lambda \vec{x}$ for some
$\lambda \in \R$,

\[
\begin{aligned}
f(\vec{x}_{\star} + \lambda \vec{x})-f(\vec{x}_{\star})) &\geq
\langle \nabla f(\vec{x}_{\star}) , \lambda \vec{x} \rangle \\
\Longrightarrow \quad
\frac{f(\vec{x}_{\star} + \lambda \vec{x})-f(\vec{x}_{\star}))}{\lambda} &\geq
\langle \nabla f(\vec{x}_{\star}) , \vec{x} \rangle \\
\end{aligned}
\]
\end{proof}
\clearpage
\begin{prob}
	(COLOR dataset with LOGClassify) See attached for source code.
\end{prob}

Output of the code, which displays the computed energies for the three values of $\lambda$, is shown below.


\begin{alltt}
	% python c3_1.py
	\( \lambda\)=0.005	final energy is 18.803242244493468
	\( \lambda\)=0.05	 final energy is 103.19933881689258
	\( \lambda\)=0.5 	 final energy is 481.6992911373476
\end{alltt}

I noticed that the results were somewhat similar to last week's least squares approximation, but overall logarithmic classification was more accurate. Smaller values of $\lambda$ provided the best extraction. The results are shown below. Higher values of lambda began to inaccurately classify the window as background: $\lambda=.05$ has some parts of the window, while $lambda=.5$ has nearly all the window as background. The rest of the image seems equivalent for all three values.
\clearpage
\begin{figure}
	\centering`
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{fg_005}
		\caption{foreground with $\lambda=.005$}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{bg_005}
		\caption{background with $\lambda = .005$ }
	\end{subfigure}
\end{figure}
\begin{figure}
		\centering`
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{fg_05}
			\caption{foreground with $\lambda=.05$}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{bg_05}
			\caption{background with $\lambda = .05$ }
		\end{subfigure}
\end{figure}

\begin{figure}
		\centering`
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{fg_5}
			\caption{foreground with $\lambda=.5$}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{bg_5}
			\caption{background with $\lambda = .5$ }
		\end{subfigure}
\end{figure}
\clearpage


\begin{prob}
	(20NEWS dataset with LOGClassify) See attached for source code.
\end{prob}

I had to make a very uninteresting changes to LOGClassify.py in order to get it to work with sparse matrices. The changes involved manipulating dimesions (for some reason numpy really doesn't like to broadcast between shapes like (61118,1) and (61118,). Great stuff. I'm sure I'll be able to merge them, but for now, for full disclosure:
\begin{verbatim}
% diff LOGClassify.py LOGClassify2.py
22c22
<     alpha, beta = 0, numpy.zeros(X.shape[1])
---
>     alpha, beta = 0, numpy.zeros((X.shape[1],1))
78c78
<     p = numpy.zeros((beta.shape[0] +1))
---
>     p = numpy.zeros((beta.shape[0] +1, 1))

\end{verbatim}
\textbf{(a-e)} \, Output of the source code is given below.

\begin{alltt}
	% python c3_2.py
--------------------------------------------------------------------------------
\(\lambda\) = 1.0
final energy with \(\lambda\)=1.0 was E=53.94825829336974
accuracy with \(\lambda\)=1.0 was 734/796 = 92.21105527638191

words found from group 10:
hockey nhl playoff penguins need beat cup playoffs wings ca

words found from group 11:
pitching cubs anyone stadium sox up nl runs phillies baseball
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
\(\lambda\) = 10.0
final energy with \(\lambda\)=10.0 was E=169.8301683152322
accuracy with \(\lambda\)=10.0 was 730/796 = 91.70854271356784

words found from group 10:
hockey nhl playoff ca cup penguins playoffs wings pens espn

words found from group 11:
yankees mets nl stadium pitching sox up phillies runs baseball
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
\(\lambda\) = 100.0
final energy with \(\lambda\)=100.0 was E=389.1747605791561
accuracy with \(\lambda\)=100.0 was 709/796 = 89.07035175879398

words found from group 10:
hockey ca nhl cup leafs playoffs go wings pens will

words found from group 11:
braves sox mets hit pitching up phillies edu runs baseball
--------------------------------------------------------------------------------
\end{alltt}

It's safe to assume that "group 10" articles are about Canadian ice-football, whereas "group 11" articles are about baseball.
\clearpage
\textbf{(f)} Using "feature extraction," I got very similar output with slightly better accuracy (around one percentage point for each $\lambda$)

\begin{alltt}
--------------------------------------------------------------------------------
\(\lambda\) = 1.0
final energy with \(\lambda\)=1.0 was E=69.9357153923059
accuracy with \(\lambda\)=1.0 was 740/796 = 92.96482412060301

words found from group 10:
hockey nhl playoff penguins cup playoffs ca beat pittsburgh wings

words found from group 11:
ball jewish stadium sox cubs nl pitching phillies runs baseball
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
\(\lambda\) = 10.0
final energy with \(\lambda\)=10.0 was E=222.9877373696374
accuracy with \(\lambda\)=10.0 was 739/796 = 92.8391959798995

words found from group 10:
hockey nhl playoff ca playoffs cup penguins wings pittsburgh goals

words found from group 11:
stadium cubs mets sox ball edu phillies pitching runs baseball
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
\(\lambda\) = 100.0
final energy with \(\lambda\)=100.0 was E=498.92152205890966
accuracy with \(\lambda\)=100.0 was 726/796 = 91.20603015075378

words found from group 10:
hockey ca nhl cup playoffs playoff wings team penguins goals

words found from group 11:
up run ball phillies mets hit pitching edu runs baseball
--------------------------------------------------------------------------------
\end{alltt}
\clearpage




\end{document}

